# -- String to partially override fullname template (will maintain the release name)
nameOverride: ""
# -- String to fully override fullname template
fullnameOverride: ""

# -- deploymentMode determines if the model will be deployed using KNative Serverless or a standard k8s Deployment.  Must be one of Serverless or RawDeployment
deploymentMode: rawDeployment

image:
  # -- The guardrails-huggingface-detector model server image
  image: 'quay.io/modh/odh-trustyai-hf-detector-runtime-rhel9'

  # -- The tag or sha for the model server image
  tag: ""

  # -- The guardrails-huggingface-detector version that will be displayed in the RHOAI Dashboard.  If not set, the appVersion of the chart will be used.
  runtimeVersionOverride: ""

model:

  # -- Additional guardrails-huggingface-detector arguments to be used to start the model.
  args: []

  # -- Additional guardrails-huggingface-detector environment variables to be used to start the model.
  env: []
    # - name: LOGGING_LEVEL
    #   value: INFO

  # -- Option to set how the storage will be configured.  Options: "uri" and "s3"
  mode: uri

  # -- The Uri to use for storage.  Mode must be set to "uri" to use this option.  Options: "oci://" and "pvc://"
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-guardian-hap-38m

  s3:
    # -- The secret containing s3 credentials.  Mode must be set to "s3" to use this option.
    key: ""

    # -- The containing the model in the s3 bucket.  Mode must be set to "s3" to use this option.
    path: ""

endpoint:
  externalRoute:
    # -- Creates an externally accessible route for the model endpoint
    enabled: true
  auth:
    # -- Secures the model endpoint and creates a role to grant permissions to service accounts
    enabled: false
    # -- Creates service accounts with permissions to access the secured endpoint
    serviceAccounts: []
      # - name: my-service-account
      # - name: existing-service-account
      #   create: false
      # - name: existing-in-another-namespace
      #   namespace: my-other-namespace
      #   create: false
      # - name: no-token
      #   createLegacyToken: false

scaling:
  # -- Sets the model server to a stopped state and spins down all pods.
  stopped: false

  # -- The minimum number of replicas to be deployed.  Set to 0 to enable scale to zero capabilities with Serverless deployments.
  minReplicas: 1

  # -- The maximum number of replicas to be deployed
  maxReplicas: 1

  # -- The scaling metric used by KServe to trigger scaling a new pod.  Serverless deployments can be "concurrency", "rps", "cpu", or "memory", while RawDeployments can only utilize "cpu" and "memory".  "concurrency" is used by default for Serverless and "cpu" is used by default for RawDeployments.
  scaleMetric: ""

  # -- The scaling target used by KNative to trigger scaling a new pod.  Default is 100 when not set.
  scaleTarget: ""

  serverless:
    # -- The timeout value determines how long before KNative marks the deployments as failed
    timeout: 30m
    # -- The retentionPeriod determines the minimum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero.
    retentionPeriod: ""

  rawDeployment:
    # -- The deployment strategy to use to replace existing pods with new ones.
    deploymentStrategy:
      # Can be type of RollingUpdate or Recreate
      type: RollingUpdate

# -- Resource configuration for the guardrails-huggingface-detector container
resources:
  requests:
    cpu: '1'
    memory: 4Gi
    nvidia.com/gpu: '1'
  limits:
    cpu: '2'
    memory: 8Gi
    nvidia.com/gpu: '1'

# -- The tolerations to be applied to the model server pod.
tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Exists

# -- Node selector for the guardrails-huggingface-detector pod
nodeSelector: {}

# -- This is for the secretes for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []

inferenceService:
  # -- Overwrite the default name for the InferenceService.
  name: ""

servingRuntime:
  # -- Overwrite the default name for the ServingRuntime.
  name: ""
  # -- Use an existing servingRuntime instead of creating one.  If useExisting value is set, no servingRuntime will be created and the InferenceService will be configured to use the value set here as the runtime name.
  useExisting: ""

  # -- The size of the emptyDir used for shared memory.  You most likely don't need to adjust this.
  shmSize: 2Gi
  # -- The arguments used to start guardrails-huggingface-detector
  args:
    - '--workers=1'
    - '--host=0.0.0.0'
    - '--port=8000'
    - '--log-config=/common/log_conf.yaml'
