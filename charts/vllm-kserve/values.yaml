# -- String to partially override fullname template (will maintain the release name)
nameOverride: ""
# -- String to fully override fullname template
fullnameOverride: ""

# -- deploymentMode determines if the model will be deployed using KNative Serverless or a standard k8s Deployment.  Must be one of Serverless or RawDeployment
deploymentMode: Serverless

# -- servingTopology determines if the model will be deployed using a single node or a multi-node topology.  Must be one of singleNode or multiNode
servingTopology: singleNode

image:
  # -- The vLLM model server image
  image: 'quay.io/modh/vllm'

  # -- The tag or sha for the model server image
  tag: rhoai-2.22-cuda

  # -- The vLLM version that will be displayed in the RHOAI Dashboard.  If not set, the appVersion of the chart will be used.
  runtimeVersionOverride: ""

model:
  # -- By default the model name will utilize the inferenceService name for the model. This parameter will override the default name to allow you to explicitly set the model name.
  modelNameOverride: ""

  # -- Additional vLLM arguments to be used to start the model.  For more documentation on available arguments see https://docs.vllm.ai/en/latest/serving/engine_args.html
  args:
    - "--gpu-memory-utilization=0.90"

  # -- Additional vLLM arguments to be used to start the model.  For more documentation on available environments variables see https://docs.vllm.ai/en/stable/serving/env_vars.html. NOTE: As of RHOAI 2.22, adding environment variables to a multi-node deployment will prevent the ray-tls-generator init container from completing successfully.
  env: []
    # - name: VLLM_LOGGING_LEVEL
    #   value: INFO

  # -- Option to set how the storage will be configured.  Options: "uri" and "s3"
  mode: uri

  # -- The Uri to use for storage.  Mode must be set to "uri" to use this option.  Options: "oci://" and "pvc://"
  uri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-2b-instruct

  s3:
    # -- The secret containing s3 credentials.  Mode must be set to "s3" to use this option.
    key: ""

    # -- The containing the model in the s3 bucket.  Mode must be set to "s3" to use this option.
    path: ""

endpoint:
  externalRoute:
    # -- Creates an externally accessible route for the model endpoint
    enabled: true
  auth:
    # -- Secures the model endpoint and creates a role to grant permissions to service accounts
    enabled: false
    # -- Creates service accounts with permissions to access the secured endpoint
    serviceAccounts: []
      # - name: my-service-account
      # - name: existing-service-account
      #   create: false
      # - name: existing-in-another-namespace
      #   namespace: my-other-namespace
      #   create: false
      # - name: no-token
      #   createLegacyToken: false

multiNode:
  # -- The number of pods to create for the multi-node topology.  Must be greater than 1.
  pipelineParallelSize: 2
  # -- The number of GPUs per node to use for the multi-node topology.
  tensorParallelSize: 1

scaling:
  # -- The minimum number of replicas to be deployed.  Set to 0 to enable scale to zero capabilities with Serverless deployments.
  minReplicas: 1

  # -- The maximum number of replicas to be deployed
  maxReplicas: 1

  # -- The scaling metric used by KServe to trigger scaling a new pod.  Serverless deployments can be "concurrency", "rps", "cpu", or "memory", while RawDeployments can only utilize "cpu" and "memory".  "concurrency" is used by default for Serverless and "cpu" is used by default for RawDeployments.
  scaleMetric: ""

  # -- The scaling target used by KNative to trigger scaling a new pod.  Default is 100 when not set.
  scaleTarget: ""

  serverless:
    # -- The timeout value determines how long before KNative marks the deployments as failed
    timeout: 30m
    # -- The retentionPeriod determines the minimum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero.
    retentionPeriod: ""



  rawDeployment:
    # -- The deployment strategy to use to replace existing pods with new ones.
    deploymentStrategy:
      # Can be type of RollingUpdate or Recreate
      type: RollingUpdate

# -- Resource configuration for the vLLM container
resources:
  requests:
    cpu: '1'
    memory: 4Gi
    nvidia.com/gpu: '1'
  limits:
    cpu: '2'
    memory: 8Gi
    nvidia.com/gpu: '1'

# -- The tolerations to be applied to the model server pod.
tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Exists

# -- Node selector for the vLLM pod
nodeSelector: {}

# -- This is for the secretes for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []

inferenceService:
  # -- Overwrite the default name for the InferenceService.
  name: ""

servingRuntime:
  # -- Overwrite the default name for the ServingRuntime.
  name: ""
  # -- Use an existing servingRuntime instead of creating one.  If useExisting value is set, no servingRuntime will be created and the InferenceService will be configured to use the value set here as the runtime name.
  useExisting: ""

  # -- The size of the emptyDir used for shared memory.  You most likely don't need to adjust this.
  shmSize: 2Gi
  # -- The arguments used to start vLLM
  args:
    - --port=8080
    - --model=/mnt/models
