# -- String to partially override fullname template (will maintain the release name)
nameOverride: ""
# -- String to fully override fullname template
fullnameOverride: ""

servingRuntime:
  # -- Use an existing servingRuntime instead of creating one.  If useExisting value is set, no servingRuntime will be created and the InferenceService will be configured to use the value set here as the runtime name.
  useExisting: ""
  # -- The vLLM model server image
  image: 'quay.io/modh/vllm@sha256:c86ff1e89c86bc9821b75d7f2bbc170b3c13e3ccf538bf543b1110f23e056316'
  # -- The size of the emptyDir used for shared memory.  You most likely don't need to adjust this.
  shmSize: 2Gi

inferenceService:
  # -- The minimum number of replicas to be deployed
  minReplicas: 1
  # -- The maximum number of replicas to be deployed
  maxReplicas: 1
  # -- The timeout value determines how long before KNative marks the deployments as failed
  timeout: 30m
  # -- Additional vLLM arguments to be used to start vLLM.  For more documentation on available arguments see https://docs.vllm.ai/en/latest/serving/engine_args.html
  args:
    - "--gpu-memory-utilization=0.90"
  # -- Additional vLLM arguments to be used to start vLLM.  For more documentation on available environments variables see https://docs.vllm.ai/en/stable/serving/env_vars.html
  env: []
  # -- Node selector for the vLLM pod
  nodeSelector: {}
  storage:
    # -- Option to set how the storage will be configured.  Options: "uri" and "s3"
    mode: uri

    # -- The Uri to use for storage.  Mode must be set to "uri" to use this option.  Options: "oci://" and "pvc://"
    storageUri: oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.0-2b-instruct

    # -- The secret containing s3 credentials.  Mode must be set to "s3" to use this option.
    key: ""

    # -- The containing the model in the s3 bucket.  Mode must be set to "s3" to use this option.
    path: ""

  # -- This is for the secretes for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  imagePullSecrets: []
  
  # -- Resource configuration for the vLLM container
  resources:
    limits:
      nvidia.com/gpu: '1'
    requests:
      nvidia.com/gpu: '1'
  
  # -- The tolerations to be applied to the model server pod.
  tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
