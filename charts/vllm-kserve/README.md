# vllm-kserve

A Helm chart for Kubernetes

![Version: 0.1.0](https://img.shields.io/badge/Version-0.1.0-informational?style=flat-square) ![Type: application](https://img.shields.io/badge/Type-application-informational?style=flat-square) ![AppVersion: v0.6.3](https://img.shields.io/badge/AppVersion-v0.6.3-informational?style=flat-square)

## Installing the Chart

To access charts from this from the cli repository add it:

```sh
helm repo add redhat-ai-services https://redhat-ai-services.github.io/helm-charts/
helm repo update redhat-ai-services
helm upgrade -i [release-name] redhat-ai-services/vllm-kserve
```

To include a chart from this repository in an umbrella chart, include it in your dependencies in your `Chart.yaml` file.

```yaml
apiVersion: v2
name: example-chart
description: A Helm chart for Kubernetes
type: application

version: 0.1.0

appVersion: "1.16.0"

dependencies:
  - name: "vllm-kserve"
    version: "0.1.0"
    repository: "https://redhat-ai-services.github.io/helm-charts/"
```

## Values

| Key | Type | Default | Description |
|-----|------|---------|-------------|
| fullnameOverride | string | `""` | String to fully override fullname template |
| inferenceService.args | list | `["--gpu-memory-utilization=0.90"]` | Additional vLLM arguments to be used to start vLLM |
| inferenceService.imagePullSecrets | list | `[]` | This is for the secretes for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ |
| inferenceService.maxReplicas | int | `1` | The maximum number of replicas to be deployed |
| inferenceService.minReplicas | int | `1` | The minimum number of replicas to be deployed |
| inferenceService.nodeSelector | object | `{}` | Node selector for the vLLM pod |
| inferenceService.resources | object | `{"limits":{"nvidia.com/gpu":"1"},"requests":{"nvidia.com/gpu":"1"}}` | Resource configuration for the vLLM container |
| inferenceService.storage.key | string | `""` | The secret containing s3 credentials.  Mode must be set to "s3" to use this option. |
| inferenceService.storage.mode | string | `"uri"` | Option to set how the storage will be configured.  Options: "uri" and "s3" |
| inferenceService.storage.path | string | `""` | The containing the model in the s3 bucket.  Mode must be set to "s3" to use this option. |
| inferenceService.storage.storageUri | string | `"oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.0-2b-instruct"` | The Uri to use for storage.  Mode must be set to "uri" to use this option.  Options: "oci://" and "pvc://" |
| inferenceService.timeout | string | `"30m"` | The timeout value determines how long before KNative marks the deployments as failed |
| inferenceService.tolerations | list | `[{"effect":"NoSchedule","key":"nvidia.com/gpu","operator":"Exists"}]` | The tolerations to be applied to the model server pod. |
| nameOverride | string | `""` | String to partially override fullname template (will maintain the release name) |
| servingRuntime.image | string | `"quay.io/modh/vllm@sha256:c86ff1e89c86bc9821b75d7f2bbc170b3c13e3ccf538bf543b1110f23e056316"` | The vLLM model server image |
| servingRuntime.useExisting | string | `""` | Use an existing servingRuntime instead of deploying one |

----------------------------------------------
Autogenerated from chart metadata using [helm-docs v1.14.2](https://github.com/norwoodj/helm-docs/releases/v1.14.2)
