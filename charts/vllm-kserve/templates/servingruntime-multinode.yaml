{{- if not (quote .Values.servingRuntime.useExisting | empty) }}
{{- if eq .Values.servingTopology "multiNode" }}
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: {{ include "vllm-kserve.servingRuntimeName" . }}
  annotations:
    {{- include "vllm-kserve.rhoaiAnnotations" . | nindent 4 }}
  labels:
    {{- include "vllm-kserve.labels" . | nindent 4 }}
    opendatahub.io/dashboard: "false"
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8080"
  containers:
    - name: kserve-container
      image: {{ include "vllm-kserve.image" . }}
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
      args:
        - --port=8080
      command:
        - bash
        - -c
        - |
          export MODEL_NAME=${MODEL_DIR}
          CMD="python3 -m vllm.entrypoints.openai.api_server \
              --distributed-executor-backend ray \
              --model=${MODEL_NAME} \
              --tensor-parallel-size=${TENSOR_PARALLEL_SIZE} \
              --pipeline-parallel-size=${PIPELINE_PARALLEL_SIZE} $0 $@"
          echo "*MultiNode VLLM Runtime Command*"
          echo "$CMD"

          echo ""
          echo "Ray Start"
          ray start --head --disable-usage-stats --include-dashboard false 
          # wait for other node to join
          until [[ $(ray status --address ${RAY_ADDRESS} | grep -c node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]; do
            echo "Waiting..."
            sleep 1
          done
          ray status --address ${RAY_ADDRESS}

          exec $CMD
      env:
        - name: RAY_USE_TLS
          value: "1"
        - name: RAY_TLS_SERVER_CERT
          value: /etc/ray/tls/tls.pem
        - name: RAY_TLS_SERVER_KEY
          value: /etc/ray/tls/tls.pem
        - name: RAY_TLS_CA_CERT
          value: /etc/ray/tls/ca.crt
        - name: RAY_PORT
          value: "6379"
        - name: RAY_ADDRESS
          value: 127.0.0.1:6379
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: VLLM_NO_USAGE_STATS
          value: "1"
        - name: HOME
          value: /tmp
        - name: HF_HOME
          value: /tmp/hf_home
      livenessProbe:
        exec:
          command:
            - bash
            - '-c'
            - |
              # Check if the registered ray nodes count is greater or the same than PIPELINE_PARALLEL_SIZE
              registered_node_count=$(ray status --address ${RAY_ADDRESS} | grep -c node_)
              if [[ ! ${registered_node_count} -ge "${PIPELINE_PARALLEL_SIZE}" ]]; then
                echo "Unhealthy - Registered nodes count (${registered_node_count}) must not be less PIPELINE_PARALLEL_SIZE (${PIPELINE_PARALLEL_SIZE})."
                exit 1
              fi     

              # Check model health
              health_check=$(curl -o /dev/null -s -w "%{http_code}\n" http://localhost:8080/health)
              if [[ ${health_check} != 200 ]]; then
                echo "Unhealthy - vLLM Runtime Health Check failed." 
                exit 1
              fi
        failureThreshold: 2
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 15
      readinessProbe:
        exec:
          command:
            - bash
            - '-c'
            - |
              # Check model health 
              health_check=$(curl -o /dev/null -s -w "%{http_code}\n" http://localhost:8080/health)
              if [[ ${health_check} != 200 ]]; then
                echo "Unhealthy - vLLM Runtime Health Check failed." 
                exit 1
              fi
        failureThreshold: 2
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 15
      startupProbe:
        exec:
          command:
            - bash
            - '-c'
            - |
              # This need when head node have issues and restarted.
              # It will wait for new worker node.                     
              registered_node_count=$(ray status --address ${RAY_ADDRESS} | grep -c node_)
              if [[ ! ${registered_node_count} -ge "${PIPELINE_PARALLEL_SIZE}" ]]; then
                echo "Unhealthy - Registered nodes count (${registered_node_count}) must not be less PIPELINE_PARALLEL_SIZE (${PIPELINE_PARALLEL_SIZE})."
                exit 1
              fi

              # Double check to make sure Model is ready to serve.
              for i in 1 2; do                    
                # Check model health
                health_check=$(curl -o /dev/null -s -w "%{http_code}\n" http://localhost:8080/health)
                if [[ ${health_check} != 200 ]]; then
                  echo "Unhealthy - vLLM Runtime Health Check failed." 
                  exit 1
                fi
              done
        failureThreshold: 40
        initialDelaySeconds: 20
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 30
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: vLLM
      priority: 2
  volumes:
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: {{ .Values.servingRuntime.shmSize }}
  workerSpec:
    pipelineParallelSize: {{ .Values.multiNode.pipelineParallelSize }}
    tensorParallelSize: {{ .Values.multiNode.tensorParallelSize }}
    containers:
      - name: worker-container
        image: {{ include "vllm-kserve.image" . }}
        command:
          - bash
          - -c
          - |
            export RAY_HEAD_ADDRESS="${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:6379"

            SECONDS=0

            while true; do
              if (( SECONDS <= 240 )); then
                if ray health-check --address "${RAY_HEAD_ADDRESS}" > /dev/null 2>&1; then
                  echo "Global Control Service (GCS) is ready."
                  break
                fi
                echo "$SECONDS seconds elapsed: Waiting for Global Control Service (GCS) to be ready."
              else
                if ray health-check --address "${RAY_HEAD_ADDRESS}"; then
                  echo "Global Control Service (GCS) is ready. Any error messages above can be safely ignored."
                  break
                fi
                echo "$SECONDS seconds elapsed: Still waiting for Global Control Service (GCS) to be ready."
                echo "For troubleshooting, refer to the FAQ at https://docs.ray.io/en/master/cluster/kubernetes/troubleshooting/troubleshooting.html#kuberay-troubleshootin-guides"
              fi

              sleep 5
            done

            echo "Attempting to connect to Ray cluster at $RAY_HEAD_ADDRESS ..."
            ray start --address="${RAY_HEAD_ADDRESS}" --block
        env:
          - name: RAY_USE_TLS
            value: "1"
          - name: RAY_TLS_SERVER_CERT
            value: /etc/ray/tls/tls.pem
          - name: RAY_TLS_SERVER_KEY
            value: /etc/ray/tls/tls.pem
          - name: RAY_TLS_CA_CERT
            value: /etc/ray/tls/ca.crt
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        livenessProbe:
          exec:
            command:
              - bash
              - -c
              - |
                # Check if the registered nodes count matches PIPELINE_PARALLEL_SIZE
                registered_node_count=$(ray status --address ${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:6379 | grep -c node_)
                if [[ ! ${registered_node_count} -ge "${PIPELINE_PARALLEL_SIZE}" ]]; then
                  echo "Unhealthy - Registered nodes count (${registered_node_count}) must not be less PIPELINE_PARALLEL_SIZE (${PIPELINE_PARALLEL_SIZE})."
                  exit 1
                fi
          failureThreshold: 2
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 15
        startupProbe:
          exec:
            command:
              - /bin/sh
              - '-c'
              - |
                registered_node_count=$(ray status --address ${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:6379 | grep -c node_)
                if [[ ! ${registered_node_count} -ge "${PIPELINE_PARALLEL_SIZE}" ]]; then
                  echo "Unhealthy - Registered nodes count (${registered_node_count}) must not be less PIPELINE_PARALLEL_SIZE (${PIPELINE_PARALLEL_SIZE})."
                  exit 1
                fi  

                # Double check to make sure Model is ready to serve.
                for i in 1 2; do
                  # Check model health
                  model_health_check=$(curl -s ${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:8080/v1/models|grep -o ${ISVC_NAME})
                  if [[ ${model_health_check} != "${ISVC_NAME}" ]]; then
                    echo "Unhealthy - vLLM Runtime Health Check failed."
                    exit 1
                  fi                     
                  sleep 10
                done
          failureThreshold: 40
          initialDelaySeconds: 20
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 30
        volumeMounts:
          - mountPath: /dev/shm
            name: shm
    volumes:
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: {{ .Values.servingRuntime.shmSize }}
{{- end }}
{{- end }}
