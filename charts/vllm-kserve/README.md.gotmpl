{{ template "chart.header" . }}
{{ template "chart.description" . }}

{{ template "chart.versionBadge" . }}{{ template "chart.typeBadge" . }}{{ template "chart.appVersionBadge" . }}

## Installing the Chart

To access charts from this from the cli repository add it:

```sh
helm repo add redhat-ai-services https://redhat-ai-services.github.io/helm-charts/
helm repo update redhat-ai-services
helm upgrade -i [release-name] redhat-ai-services/{{ template "chart.name" . }}
```

To include a chart from this repository in an umbrella chart, include it in your dependencies in your `Chart.yaml` file.

```yaml
apiVersion: v2
name: example-chart
description: A Helm chart for Kubernetes
type: application

version: 0.1.0

appVersion: "1.16.0"

dependencies:
  - name: "{{ template "chart.name" . }}"
    version: "{{ template "chart.version" . }}"
    repository: "https://redhat-ai-services.github.io/helm-charts/"
```

## Usage

### Deploying a Model with ModelCar (OCI)

The chart provides the ability to deploy models via URI or S3.  To deploy a model from an OCI container (aka ModelCar) you must set the storage mode to `uri` and then provide the OCI URI.

```sh
helm upgrade -i [release-name] redhat-ai-services/{{ template "chart.name" . }} \
  --set inferenceService.storage.mode=uri \
  --set inferenceService.storage.storageUri="oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.0-2b-instruct"
```

### Deploy a Model with S3

To deploy a model from an S3 bucket, a secret containing the connection details in the OpenShift AI S3 Data Connection format must exist.

```yaml
kind: Secret
apiVersion: v1
metadata:
  name: my-s3-connection
  labels:
    opendatahub.io/dashboard: 'true'
    opendatahub.io/managed: 'true'
  annotations:
    opendatahub.io/connection-type: s3
    opendatahub.io/connection-type-ref: s3
    openshift.io/description: ''
    openshift.io/display-name: my-s3-connection
type: Opaque
data:
  AWS_ACCESS_KEY_ID: YmxhaA==
  AWS_S3_BUCKET: YmxhaA==
  AWS_S3_ENDPOINT: YmxhaA==
  AWS_SECRET_ACCESS_KEY: YmxhaA==
```

To configure the S3 option, you must set the storage mode to S3 and provide the name of the secret for the S3 Data Connection and the path the model exists in the bucket.  The model cannot be located in the root folder of the bucket.

```sh
helm upgrade -i [release-name] redhat-ai-services/{{ template "chart.name" . }} \
  --set inferenceService.storage.mode=s3 \
  --set inferenceService.storage.key="my-s3-connection" \
  --set inferenceService.storage.path="my-model-folder"
```

### Setting Additional Arguments

Many models may require additional arguments to be configured in order to successfully start.  Additional arguments can be set with the following option:

```sh
helm upgrade -i [release-name] redhat-ai-services/{{ template "chart.name" . }} \
  --set inferenceService.args={"--gpu-memory-utilization=0.95", "--max-model-len=10000"}
```

For more information on available arguments, see the [vLLM Engine Arguments](https://docs.vllm.ai/en/latest/serving/engine_args.html
) documentation.

### Securing the Endpoint

By default the vLLM instance is not secured with authentication, but this feature can be enabled by setting the following options:

```sh
helm upgrade -i [release-name] redhat-ai-services/{{ template "chart.name" . }} \
  --set endpoint.auth.enabled=true 
```

To access the endpoint, a user token must be provided uses a `Bearer token`.  The user must have view access to the `InferenceService` instance.  

For normal OpenShift users, you can utilize the same sha token used when logging in via `oc` if your user has access as the `Bearer token`, but this is not advised for long term solutions as the `oc` token expires after 24 hours by default.

Instead, you can create a Service account with the correct permissions using the following:

```sh
helm upgrade -i test charts/vllm-kserve \
  --set endpoint.auth.enabled=true \
  --set 'endpoint.auth.serviceAccounts[0].name=my-service-account'
```

This option will create the serviceAccount `my-service-account` and a secret with the matching name that includes an automatically generated token value.  This token is generated using a legacy k8s token tool.

>[!WARNING]
> Service Accounts created through this helm chart or service accounts that are granted permission to the vLLM instance are not visible through the OpenShift AI UI.

The `serviceAccounts` configuration supports several useful options:

```
endpoint:
  auth:
    serviceAccounts:
      - name: my-service-account
      - name: existing-service-account
        create: false
      - name: existing-in-another-namespace
        namespace: my-other-namespace
        create: false
      - name: no-token
        createLegacyToken: false
```

The `create` option defaults to true if not set.  Disabling `create` can be useful if you wish to grant permissions to a serviceAccount that already exists.

The `namespace` option allows you to specify which namespace the service account exists or should be created.  Generally it is recommended to only use this feature with `create: false` to grant permissions to an existing service account and not create a new one in that namespace.  If namespace is not specified it will utilize the Release Namespace by default.

The `createLegacyToken` defaults to true if not set.  This option allows you to disable the creation of a legacy k8s token when creating a new serviceAccount and instead to rely on automounted tokens in k8s.  See the official k8s [Service Account documentation](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#auto-generated-legacy-serviceaccount-token-clean-up) for more information.

{{ template "chart.sourcesSection" . }}

{{ template "chart.requirementsSection" . }}

{{ template "chart.valuesSection" . }}

{{ template "helm-docs.versionFooter" . }}
